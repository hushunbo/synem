\section{Results}

\subsection{Mono-modal registration}
Although our algorithms were designed for multi-modal image registration, it is important to first verify that the quality of the algorithms is reasonable for mono-modal registration. Table \ref{tab:monomodal_results_seg} and Figure \ref{fig:mono_graph_seg} show the average overlap score for each of 31 manually annotated anatomical regions from the IBSR database. Note that the Jaccard indices obtained with CC (i.e. ANTS) are higher than reported by Rohlfing \cite{Rohlfing2012}. In his experiments, he used three resolutions with a maximum of 10, 10 and 5 iterations only. Here, we set a maximum of 100, 100 and 25 iterations, leaving the rest of the parameters unchanged. We can see that SyN with the EM metric is very competitive, but still not as good as CC. This may be explained by the fact that CC uses a window centered at each voxel for computing the similarity, while the EM is voxelwise. This behavior can also be observed from the results of SyN with Mutual Information (MI), which is also voxelwise and very competitve but not as good as CC. By considering neighborhoods of the same size, the performance of ECC is practically the same as CC. Table \ref{tab:monomodal_results_segTri_fill} shows the overlap scores over tissue types (white matter, gray matter and cerebrospinal fluid), instead of anatomical areas.

\input{monomodal_results_seg}
\input{monomodal_results_segTri_fill}

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{./images/mono_lines_seg.png}\\
\caption{Comparison of the registration performance (measured by the Jaccard index over 31 anatomical regions) of the Greedy SyN algorithm with EM, ECC, CC and MI metrics. The Jaccard
indices were averaged over 306 monomodal registrations.}
\label{fig:mono_graph_seg}
\end{figure}

\subsection{Multi-modal registration}\label{sec:multimodal_results}
The cross-correlation metric is occasionally used for some multi-modal image registration tasks. The reason is that the metric's optimality is reached when the relationship between the two image modalities is locally affine. However, in general, this condition is not met. To illustrate this, we used the Brainweb synthetic template \citep{Cocosco1997, Kwan1999}. Brainweb provides two perfectly aligned, realistically simulated brain MR images in T1 and T2 modalities. Ideally, a multi-modal registration method should return the identity transformation to register these perfectly aligned images. Figure \ref{fig:t1_t2_reg_pannel} depicts the result of registering the images using the SyN algorithm with four different metrics. We can see that using the CC metric, the cortex was (incorrectly) strongly deformed: the deformation field maximizing the cross correlation between the two multi-modal images does not coincide with the deformation field correctly aligning the images. This effect is less pronounced using the EM metric. Despite the deformation field is not exactly zero using the ECC metric, the deformations are much smaller. Using MI, the moving image is left almost (Table \ref{tab:deformation_magnitude}). The reason that the CC metric does not perform well in this experiment is that the relationship between the two modalities is not locally affine (Fig. \ref{fig:transfers}). Despite the MI metric successfully reaches its maximum very close to the identity transformation, we must consider that it depends on an accurate computation of the joint and marginal intensity distributions of both modalities, which is not known {\it a priori} and needs to be iteratively refined as the registration evolves. By providing two perfectly aligned images as input, the intensity distributions can be accurately computed from iteration one, which is an unrealistic scenario. Thus, a more realistic validation protocol is required to quantitatively evaluate the metrics under comparison in the multi-modal case.

\begin{table}[htbp]
  \centering
  {\small
    \begin{tabular}{ccccc}
    \toprule
    \textbf{} & \textbf{SyN-CC} & \textbf{SyN-EM} & \textbf{SyN-ECC} &\textbf{SyN-MI} \\
    \midrule
    \textbf{Mean}    & 2.98  & 0.21 & 0.15 & 0.06 \\
    \textbf{Maximum} & 11.70 & 3.90 & 2.60 & 0.40\\
    \bottomrule
    \end{tabular}%
    \caption{Magnitude (in millimeters) of the deformation field obtained by registering two perfectly aligned multi-modal images using SyN with four different metrics (ideally, the deformation field should be zero). First row shows the average norm of all displacement
    vectors, while second row shows the maximum norm.}
  \label{tab:deformation_magnitude}}%
\end{table}%


\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{./images/t1_t2_reg_pannel.png}
\caption{T2 brain image registered towards T1 using the greedy SyN algorithm with different metrics. Since both images are perfectly aligned, the expected result is a zero deformation field. Row 1: using the CC metric, the cortex is clearly distorted. Row 2: with the EM metric deformations are less visible. Row 3: with the ECC metric deformations are smaller than with the EM metric. Row 4: the MI metric leaves the image almost unchanged (see Table \ref{tab:deformation_magnitude} for detailed numerical results). Note that, with exception of the CC metric, the largest deformations occur outside of the region of interest, which is a result of the regularization term.}
\label{fig:t1_t2_reg_pannel}
\end{figure}

Unfortunately, to the best of our knowledge, there are no manually annotated multi-modal data publicly available to evaluate the registration performance quantitatively. Instead, we generated synthetic T2 images for all IBSR T1 images by following the procedure illustrated in Fig. (\ref{fig:semi_synthetic_image_creation}). We first register the Brainweb T1 template (which plays the role of moving image) towards each IBSR T1 image (which play the role of fixed images) using ANTs with the CC metric (a mono-modal registration problem), which has consistently been among the top performers in the most extensive evaluations of mono-modal registration algorithms \citep{Klein2009, Klein2010, Rohlfing2012}. Then we applied the obtained deformation field to the Brainweb T2 template. The transfer function from the real T1 image to the warped T2 template is computed as the average T2 intensity associated to each T1 intensity. The computed transfer function is then applied to the real IBSR T1 image obtaining a ``perfectly aligned'' realistic synthetic T2 image for each IBSR volume, therefore the annotations remain exactly the same as the T1 counterparts and we are able to compute the overlap scores as before. Note that the number of registrations we need to perform is now 612 because we can use the T2 modality either as the moving or the fixed image. Table \ref{tab:multimodal_results_seg} and Figure \ref{fig:multi_seg} show results similar to Table \ref{tab:monomodal_results_seg} and Figure \ref{fig:mono_graph_seg} but this time averaged over 612 multimodal registrations. Note how the performance of the CC metric strongly drops while EM, ECC and MI are less affected by the change of modality. Table \ref{tab:multimodal_results_segTri_fill} shows the overlap scores of tissue types, where the same behavior can be observed.\\

\begin{figure}[t!]
\centering
    \includegraphics[width=\linewidth]{./images/semi_synthetic_image_creation.png}
    \caption{Semi-synthetic, manually annotated images for quantitative evaluation of multi-modal non-linear image registration algorithms.}
\label{fig:semi_synthetic_image_creation}
\end{figure}

The Brainweb template also provides us with a simulation of a proton density (PD) image for the same brain anatomy. This allows us to repeat the procedure described above for each pair of three modalities, T1, T2 and PD, which results in 3 mono-modal and 3 multi-modal registration experiments. For the sake of brevity, Figure \ref{fig:all_pairs_boxplots} depicts a summary of the results obtained over all pairs of modalities. For each pair of registered images, we compute the average Jaccard index of all anatomical regions. Each boxplot in Figure \ref{fig:all_pairs_boxplots} corresponds to the set of all 306 (mono-modal) or 612 (multi-modal) average scores for that particular method and that particular pair of modalities. We can observe that the CC metric performs very well for all mono-modal experiments (diagonal graphs), but it is severely affected in the multi-modal case (off-diagonal graphs). The ECC metric performs practically the same as CC in the mono-modal case, but is less affected in the multi-modal case. By measuring the similarity of local windows instead of individual voxels, the ECC metric outperforms the EM and MI metrics in all cases.

\begin{figure}[t!]
\centering
    \subfloat[]{\label{fig:T1T2}\includegraphics[width=0.5\linewidth]{./images/t1_t2_transfer_brainweb.png}}
    \subfloat[]{\label{fig:T2T1}\includegraphics[width=0.5\linewidth]{./images/t2_t1_transfer_brainweb.png}}\\
    \caption{Estimated transfer functions, according to eq. \eqref{eq:expectation_transfer} using the Brainweb template where the T1 template is $I$ and the T2 template is $J$
    (since the two templates are perfectly aligned, the transformations $\phi_{I}, \phi_{J}$ are identities). (a) Estimated $\overline{Y}(x), 0\leq x \leq 255$, (b) Estimated $\overline{Z}(x), 0\leq x \leq 255$. The height of each box is proportional to $\sigma_{Y}$ and $\sigma_{Z}$, respectively. The fact that the variances are not zero indicates that there is no exact transfer function between the two modalities. Larger variances indicate larger uncertainty in the estimated transfer function, which is used by the EM metric to weigh the observed intensities. These transfers correspond to the T1-T2 template images and are used for illustrative purposes only. To generate the semi-synthetic images used for quantitative validation, a different transfer must be computed from each IBSR T1 image to the template T2 image (see Fig. \ref{fig:semi_synthetic_image_creation}).}
\label{fig:transfers}
\end{figure}


%\begin{figure}[H]
%\centering
%\includegraphics[width=1.0\linewidth]{./images/semi_synthetic.png}\\
%\caption{First 9 T2 images generated from the real IBSR T1 images and the T2 Brainweb template (see text). Since the anatomy remained unchanged, the manual annotations are exactly %the same as the corresponding T1 images. Notice that, since several T1 intensities map to the same T2 intensity, the transfer from T2 to T1 is not monomodal (see Fig. %\ref{fig:transfers} for an example of the transfers' shape).}
%\label{fig:semi_synthetic}
%\end{figure}

\input{multimodal_results_seg}
\input{multimodal_results_segTri_fill}

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{./images/multi_lines_seg.png}\\
\caption{Comparison of the registration performance (measured by the Jaccard index over 31 anatomical regions) of the Greedy SyN algorithm with EM, ECC, CC and MI metrics. The Jaccard
indices were averaged over 612 multimodal registrations.}
\label{fig:multi_seg}
\end{figure}

\begin{figure}[t!]
\centering
    \includegraphics[width=\linewidth]{./images/all_modality_pairs_boxplots.png}
    \caption{Distributions of average Jaccard indices for all pairs of available modalities (T1, T2 and PD). For each pair of registrations, we compute the average Jaccard index of all anatomical regions. The boxplots show the distribution of all such average overlap scores.}
\label{fig:all_pairs_boxplots}
\end{figure}
