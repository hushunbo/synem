\appendix
\section{Computation of the E-Step}\label{ap:E_step}
Here, we show the details to compute the E-step of the EM algorithm \cite{Dempster1977} to obtain a local maximum likelihood estimator for $\Phi = (\phi_{I}, \phi_{J})$.
We follow similar steps as in Arce {\it et al.} \cite{Arce-santana2014}.\\

The E-step \cite{Dempster1977} consists in computing the conditional expectation of the log-likelihood of the (complete) data $(Y, Z, \tilde{I}, \tilde{J}, \Phi)$, according to the observation model given in eq. \eqref{eq:SyNEM_gom_update}, where $Y, Z$ are hidden random fields modeling the (unknown) transfer functions between both modalities \hbox{(eq. \eqref{eq:hidden_fields})}. A consequence of the transformations $\phi_{I}, \phi_{J}$ being diffeomorphisms (in particular, invertible) is that the domains $\Omega_{I}, \Omega_{J}$ satisfy \hbox{$\phi_{I}(\Omega_{I}) = \Omega_{R} = \phi_{J}(\Omega_{J})$} (see fig. \ref{fig:syn_overview}). This implies that $\tilde{I}$ is independent from $\phi_{I}$, because the event $\left\lbrace I(\phi_{I}^{-1}(x)) : x\in \Omega_{R} \right\rbrace$ is equal to the event $\left\lbrace I(y) : y\in \Omega_{I} \right\rbrace$, therefore:
\begin{equation}\label{eq:image_transform_independence}
     P(\tilde{I} | \Phi) = P(I | \Phi) = P(I).
\end{equation}
A weaker assumption was made in \cite{Roche2000} (p. 73), to obtain a similar independence property (they only require the transformation to be injective, not necessarily invertible).\\

We will assume that, when the transformations $\Phi=(\phi_{I}, \phi_{J})$ are known, the random fields $(Y, \tilde{I})$ are independent from $(Z, \tilde{J})$. In other words, once we know $\Phi$, any knowledge of $(Z, \tilde{J})$ no longer provide any additional information regarding the distribution of $(Y, \tilde{I})$, and viceversa. More precisely:
\begin{equation}\label{eq:conditional_independence_assumption}
    P(\tilde{I}, \tilde{J}, Y, Z | \Phi) = P(\tilde{I}, Y | \Phi)P(\tilde{J}, Z| \Phi).
\end{equation}
From eq. \eqref{eq:image_transform_independence}, it follows that
\begin{equation}\label{eq:joint_to_conditional}
    P(\tilde{I}, Y | \Phi) = P(Y | \tilde{I}, \Phi)P(\tilde{I} | \Phi) = P(Y | I, \Phi)P(I),
\end{equation}
similarly, $P(\tilde{J}, Z| \Phi) = P(Z| J, \Phi)P(J)$. From eqs. \eqref{eq:conditional_independence_assumption} and \eqref{eq:joint_to_conditional} we obtain
\begin{equation}\label{eq:simplified_joint_prob}
    P(\tilde{I}, \tilde{J}, Y, Z, \Phi) = P(Y | I, \Phi)P(Z| J, \Phi)P(\Phi)P(I)P(J).
\end{equation}

The conditional expectation of the log-likelihood (of the complete data) evaluated on an initial parameter $\Phi^{(0)}$ can be simplified using eq. \eqref{eq:simplified_joint_prob} as follows:
\begin{displaymath}
    Q(\Phi; \Phi^{(0)}) := \mathbbm{E}\left[\left. \log \left(P(\tilde{I}, \tilde{J}, Y, Z, \Phi)\right) \right | I, J, \Phi^{(0)}\right] =
\end{displaymath}
\begin{displaymath}
    \mathbbm{E}\left[\left. \log \left(P(Y |I, \Phi)P(Z|J, \Phi)P(\Phi)P(I)P{J}\right) \right | I, J, \Phi^{(0)}\right] =
\end{displaymath}
\begin{equation}\label{eq:separated_expectations}
    -R(\Phi) + C + \mathbbm{E}\left[ \log \left(P(Y|I, \Phi)\right) | I, J, \Phi^{(0)}\right] + \mathbbm{E}\left[ \log \left(P(Z|J, \Phi)\right) | I, J, \Phi^{(0)}\right]
\end{equation}
where $R(\Phi) = -\log P(\Phi)$ is known as the ``regularization term'', and \hbox{$C=\log \left(P(I)P(J)\right)$} is a constant that does not depend on $\Phi$. For simplicity, we will elaborate on the first expectation only, the second can be computed analogously.\\

By definition, the random field $\eta_{J}$ is a set of independent, zero-mean, Normally-distributed random variables. More precisely, if $f_{x}:\mathbf{R}\rightarrow \mathbf{R}^{+}$ is the probability density
function of $\eta_{J}(x)$ then:
\begin{equation}\label{eq:gaussian}
    f_{x}(u) = \frac{1}{\sigma_{Y}(x)\sqrt{2 \pi}}\exp\left(-\frac{u^{2}}{2\sigma^{2}_{Y}(x)}\right)
\end{equation}
and, from our observation model (eq. \eqref{eq:SyNEM_gom_update}), it follows that:
\begin{equation}
    P(Y(x)| I, \Phi) = P(\eta_{J}(x) = Y(x)-\tilde{I}(x)) = f_{x}(Y(x)-\tilde{I}(x)).
\end{equation}
Therefore, the joint conditional probability of $Y$ given $I, \Phi$ is the product of the above marginals:
\begin{equation}\label{eq:Y_field_cond_indep}
    P(Y|I, \Phi) = \prod_{x\in{\Omega_{R}}} f_{x}(Y(x) - \tilde{I}(x)),
\end{equation}
and the first conditional expectation of eq. \eqref{eq:separated_expectations} can be writen as:
\begin{equation}
    Q_{I}(\phi_{I} ; \Phi^{(0)}) := \int_{\mathcal{Y}}\left[\sum_{x\in\Omega_{R}} \log f_{x}\left(y(x) - \tilde{I}(x)\right) \right] dP(y | I, J, \Phi^{(0)}),
\end{equation}
where $\mathcal{Y}$ is the set of all possible configurations of the vector field $Y$. By assuming a finite grid $\Omega_{R}$, we may interchange the order of summation and integration, which yields:
\begin{equation}\label{eq:data_term}
    Q_{I}(\phi_{I} ; \Phi^{(0)}) = \sum_{x\in\Omega_{R}} \int_{\mathcal{Y}} \log f_{x}\left(y(x) - \tilde{I}(x)\right)  dP(y | I, J, \Phi^{(0)}).
\end{equation}

Since all the variables $Y(x), x\in\Omega_{R}$ are conditionally independent given $I, \Phi^{(0)}$ (eq. \eqref{eq:Y_field_cond_indep}), we have $P(y | I, J, \Phi^{(0)}) = P(y(x)| I, J, \Phi^{(0)})P(y^{C}(x) | I, J, \Phi^{(0)})$, where the complement $Y^{C}(x)$
denotes the subset of random variables other than $Y(x)$, and $y^{C}(x)$ is a configuration of $Y^{C}(x)$. Therefore, the right-hand side of eq.\eqref{eq:data_term} can be written as

\begin{equation}\label{eq:split_integral}
    \sum_{x\in\Omega_{R}} \int_{\mathcal{Y}^{C}(x)} \left[\int_{\mathbf{R}} \log f_{x}\left(\ell - \tilde{I}(x)\right) P(Y(x) = \ell | I, J, \Phi^{(0)})d\ell\right]  dP(y^{C}(x) | I, J, \Phi^{(0)})
\end{equation}
where $\mathcal{Y}^{C}(x)$ is the set of all possible configurations of $Y^{C}(x)$.\\

Since \hbox{$\int_{\mathcal{Y}^{C}(x)}dP(y^{C}(x) | I, J, \Phi^{(0)}) = 1$}, and the inner integral
is an expected value w.r.t. the conditional distribution:
\begin{equation}
     Q_{I}(\phi_{I} ; \Phi^{(0)}) = \sum_{x\in\Omega_{R}} \mathbbm{E} \left[\left.\log f_{x}\left(Y(x) - \tilde{I}(x)\right) \right| I, J, \Phi^{(0)}\right].
\end{equation}
By expanding the density function (according to eq. \eqref{eq:gaussian}) and denoting by $\overline{Y}(x), \widehat{\sigma^{2}_{Y}(x)}$ the conditional mean and variance of $Y(x)$
given the data and fixed parameters $\Phi^{(0)}$, we have
\begin{equation}
    \mathbbm{E} \left[\left.\log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \frac{(Y(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}\right|I, J, \Phi^{(0)} \right] =
\end{equation}
\begin{equation}
    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \mathbbm{E} \left[\left. \frac{(Y(x) - \overline{Y}(x) + \overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}\right|I, J, \Phi^{(0)} \right]=
\end{equation}
%\begin{equation}
%    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \mathbbm{E} \left[\left. \frac{(Y(x) - \overline{Y}(x))^{2}+ (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2} - 2(Y(x) - \overline{Y}(x)) (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))}{2\sigma^{2}_{Y}(x)}\right|I, J, \Phi^{(0)} \right]=
%\end{equation}
\begin{equation}
    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \frac{\widehat{\sigma^{2}_{Y}(x)} + (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}.
\end{equation}
Finally, by approximating the (unknown) parameters $\sigma_{Y}(x)$ by $\widehat{\sigma_{Y}(x)}$ we obtain:
\begin{equation}
    Q_{I}(\phi_{I} ; \Phi^{(0)}) = C_{I} - \sum_{x\in\Omega_{x}}\frac{(\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\widehat{\sigma^{2}_{Y}(x)}},
\end{equation}
where $C_{I} = -\frac{1}{2} - \log\left(\widehat{\sigma^{2}_{Y}(x)}\sqrt{2\pi}\right)$ is a constant that depends on the (known) initial transformations $\Phi^{(0)}$, but not on the (unknown) transformation $\phi_{I}$ we wish to optimize. Image $\overline{Y}$ may be regarded as an approximation of image $\tilde{I}$, since it can be efficiently computed by assigning to $\overline{Y}(x)$ the average intensity of the iso-set $\left\lbrace \tilde{I}(y) : \tilde{J}(y) = \tilde{J}(x)\right\rbrace$ \cite{Roche1998}. The conditional variance $\widehat{\sigma^{2}_{Y}}(x)$ measures the uncertainty at each point $x\in \Omega_{R}$.\\

Analogously, if $\overline{Z}(x), \widehat{\sigma^{2}_{Z}(x)}$ are the conditional mean and variance of $Z(x)$ given the data
and fixed parameters $\Phi^{(0)}$, the second conditional expectation of eq. \eqref{eq:separated_expectations} can be writen as

\begin{equation}
    Q_{J}(\phi_{J}; \Phi^{(0)}) = C_{J} - \sum_{x\in\Omega_{x}}\frac{(\overline{Z}(x) - J(\phi^{-1}_{J}(x)))^{2}}{2\widehat{\sigma^{2}_{Z}(x)}}.
\end{equation}
If we choose a regularization function of the form $R(\Phi) = \lambda R_{I}(\phi_{I}) + \lambda R_{J}(\phi_{J})$ the final function to be \textbf{maximized} in the M step is given by

\begin{equation}
    Q(\Phi; \Phi^{(0)}) = \left[Q_{I}(\phi_{I}; \Phi^{(0)}) - \lambda R_{I}(\phi_{I})\right] + \left[Q_{J}(\phi_{J}; \Phi^{(0)}) - \lambda R_{J}(\phi_{J})\right],
\end{equation}
which is the sum of two independent cost functions for $\phi_{I}$ and $\phi_{J}$.

\pagebreak
\section{Gradient of the CC metric}\label{ap:CC_gradient}
We are interested in computing the gradient of
\begin{equation}
    CC(\bar{I}, \bar{J}, x) = \frac{<\bar{I}, \bar{J}>^{2}}{<\bar{I}><\bar{J}>}
\end{equation}
where the inner product is taken over an $n^{D}$ window (eq. 4, in Avants {\it et al.} \cite{Avants2008}). Since the windows are considered discrete, a more precise notation
for this expression is:
\begin{equation}\label{eq:CC_definition}
    CC(y;\phi_{I}, \phi_{J}) = \frac{\left[\sum_{z\in W_{y}} \left(\tilde{I}(z) - \mu_{y}\right)\left(\tilde{J}(z) - \nu_{y}\right)\right]^{2}}
    {\left[\sum_{z \in W_{y}}\left(\tilde{I}(z) - \mu_{y}\right)^{2}\right] \left[\sum_{z \in W_{y}}\left(\tilde{J}(z) - \nu_{y}\right)^{2}\right]} = \frac{A_{y}^{2}}{B_{y}C_{y}}
\end{equation}
where $\tilde{I}(z) = I(\phi_{I}^{-1}(z))$, $\tilde{J}(z) = J(\phi_{J}^{-1}(z))$ and the full energy is given by
\begin{equation}
    CC(\phi_{I}, \phi_{J}) = \sum_{y\in\Omega} CC(y; \phi_{I}, \phi_{J})
\end{equation}
where $W_{y}$ is the window of side $n$ centered at voxel $y$, $|W_{y}|$ is the number of voxels in window $W_{y}$ and:
\begin{equation}
    \begin{array}{lll}
        \mu_{y} &=& \frac{1}{|W_{y}|}\sum_{z \in W_{y}}\tilde{I}(z)\\
        \nu_{y} &=& \frac{1}{|W_{y}|}\sum_{z \in W_{y}}\tilde{J}(z)\\
    \end{array}.
\end{equation}

We wish to compute the gradient of $CC(\phi_{I}, \phi_{J})$ with respect to $\phi^{-1}_{J}(x)$, $x\in\Omega$. The set of window terms $CC(y;\phi_{I}, \phi_{J})$
that depend on $\phi^{-1}_{J}(x)$ is precisely the set of all windows $W_{y}$ such that $y \in W_{x}$. Therefore:
\begin{equation}
    \frac{\partial CC (\phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} = \sum_{y \in W_{x}} \frac{\partial CC (y; \phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)}
\end{equation}
and
\begin{equation}
    \frac{\partial CC (y; \phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} =
        \frac{\left(2A_{y} B_{y}C_{y}\right)\frac{\partial A_{y}}{\partial \phi^{-1}_{J}(x)} - \left(A_{y}^{2}B_{y}\right)\frac{\partial C_{y}}{\partial \phi^{-1}_{J}(x)}}
             {B_{y}^{2} C_{y}^{2}}
\end{equation}
where
\begin{equation}
    \begin{array}{lll}
        \frac{\partial A_{y}}{\partial \phi^{-1}_{J}(x)} &=& (\tilde{I}(x) - \mu_{y})\nabla \tilde{J}(x)\\
        \frac{\partial C_{y}}{\partial \phi^{-1}_{J}(x)} &=& 2(\tilde{J}(x) - \nu_{y})\nabla \tilde{J}(x)
    \end{array}.
\end{equation}
After simplifying, we get
\begin{equation}\label{eq:CC_gradient}
    \frac{\partial CC (\phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} = \sum_{y \in W_{x}}
         \frac{2A_{y}}
              {B_{y}C_{y}}\left[ (\tilde{I}(x) - \mu_{y}) - \frac{A_{y}}{C_{y}}\left(\tilde{J}(x) - \nu_{y}\right)\right]\nabla \tilde{J}(x).
\end{equation}
If we define the scalar functions
\begin{equation}
    \begin{array}{lll}
        S_{a}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}}{B_{y}C_{y}}\\
        S_{b}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}^{2}}{B_{y}C_{y}^{2}}\\
        S_{c}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}}{B_{y}C_{y}} \left[ \mu_{y} - \frac{A_{y}}{C_{y}}\nu_{y}\right].
    \end{array}
\end{equation}
we can write the gradient of $CC$ with respect to $\phi^{-1}_{J}$ as:

\begin{equation}
    \psi_{J} = \nabla_{\phi^{-1}_{J}} CC(\phi_{I}, \phi_{J}) = \left[S_{a} \tilde{I} - S_{b}\tilde{J} - S_{c}\right]\nabla \tilde{J}.
\end{equation}

It is important to notice the similarity and differences between equation \eqref{eq:CC_gradient} and the expressions derived by Hermosillo {\it et al.} \cite{Hermosillo2004}
and Avants {\it et al.} \cite{Avants2008}. On the one hand, Hermosillo {\it et al.} considered separately the cases of local and global intensity comparisons: the global comparison corresponds to setting $W_{x} = \Omega$ (summing over the full domain). The local intensity comparison is accomplished by using a Gaussian kernel centered at each location $x\in\Omega$. The limitation of using Gaussian kernels is the computational cost, as pointed out by Hermosillo {\it et al.} \cite{Hermosillo2004},
which makes it necessary to perform some simplifications and use parallel computing, while using rectangular windows results in a very efficient and robust implementation, as shown by Avants {\it et al.}\cite{Avants2008}. On the other hand, even though the formulation of Avants {\it et al.}\cite{Avants2008}
is the same as eq. \eqref{eq:CC_definition}, only the central voxel contributes to the sum in eq. \eqref{eq:CC_gradient} (see Avants {\it et al.} \cite{Avants2008}, equations (6) and (7)). In practice, we have not observed significant differences between the quantitative results using eq. \eqref{eq:CC_gradient} of this appendix, and eqs. (6) and (7) from Avants {\it et al.} \cite{Avants2008}, thus we opted for using rectangular windows and the gradient computations as in \cite{Avants2008}.

