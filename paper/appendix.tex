\appendix
\section{Computation of the E-Step}\label{ap:E_step}
Here, we show the details to compute the E-step of the EM algorithm \cite{Dempster1977} to obtain a local maximum likelihood estimator for $\phi_{I}, \phi_{J}$
according to the observation model in eq. (\ref{eq:SyNEM_gom_update}). We follow similar steps as in Arce et al. \cite{Arce-santana2014}.\\

The E-step \cite{Dempster1977} consists in computing the conditional expectation of the log-likelihood of the (complete) data $(Y, Z, I, J)$ according to the observation model given
in eq. \ref{eq:SyNEM_gom_update}. The conditional expectation is to be computed given initial approximations of the transformations
$\phi^{(0)} = \left(\phi_{I}^{(0)}, \phi_{J}^{(0)}\right)$ and the (observed) data $I, J$. For simplicity, we will compute the E-step corresponding to the hidden random field $Y$.
The cost function associated to the hidden field $Z$ can be obtained analogously.\\

%In this section, we will compute the EM steps with respect to $Y$ and $\phi_{I}$. The corresponding steps with respect to $Z$ and $\phi_{J}$ are analogous. Our observations are
%the input images $I, J$, which take values on the set of intensities $G$. The parameters of our model, which we would like to optimize is the diffeomorphism $\phi_{I}$.
The random field $\eta_{J}$ is a set of independent Normally-distributed random variables. More precisely, if $f_{x}:\mathbf{R}\rightarrow \mathbf{R}^{+}$ is the probability density
function of $\eta_{J}(x)$ then:
\begin{equation}\label{eq:gaussian}
    f_{x}(u) = \frac{1}{\sigma_{Y}(x)\sqrt{2 \pi}}\exp\left(-\frac{u^{2}}{2\sigma^{2}_{Y}(x)}\right)
\end{equation}
and the marginal distribution of each hidden variable $Y(x)$ given the data is a function of $\phi_I$:
\begin{equation}
    P(Y(x) = \ell | I, J, \phi_{I}, \phi_{J}) = f_{x}(\ell - I(\phi^{-1}_{I}(x))).
\end{equation}

The conditional expectation of the log-likelihood with respect to the above conditional distribution evaluated on an initial parameter $\phi^{(0)}$ is:

\begin{equation}
    Q(\phi_{I}; \phi^{(0)}) = \mathbbm{E}\left[ \log \left(P(I, J, Y|\phi_{I})P(\phi_{I})\right) | I, J, \phi^{(0)}\right] =
\end{equation}

\begin{equation}
    =-R(\phi_{I}) + \int_{\mathcal{Y}}\left[\sum_{x\in\Omega_{R}} log f_{x}\left(y(x) - I(\phi^{-1}_{I}(x))\right) \right] dP(y | I, J, \phi^{(0)}).
\end{equation}

Where $\mathcal{Y}$ is the set of all possible configurations of the vector field $Y$. The term $R(\phi_{I}) = -\log P(\phi_{I})$ is known as the ``regularization term'' and comes out of the
integral because it does not depend on $Y$. We will disregard this term for now and concentrate on the second term, known as the ``data term'', in which $\phi_{I}$ is the
set of parameters we wish to optimize. By assuming a finite grid $\Omega_{R}$, we may interchange the order of summation and integration, which yields the data term:
\begin{equation}\label{eq:data_term}
    \sum_{x\in\Omega_{R}} \int_{\mathcal{Y}} log f_{x}\left(y(x) - I(\phi^{-1}_{I}(x))\right)  dP(y | I, J, \phi^{(0)}).
\end{equation}

Since all the variables $Y(x), x\in\Omega_{R}$ are independent, $P(y | I, J, \phi^{(0)}) = P(y(x)| I, J, \phi^{(0)})P(y^{C}(x) | I, J, \phi^{(0)})$, where the complement $Y^{C}(x)$
denotes the subset of random variables other than $Y(x)$, and $y^{C}(x)$ is a configuration of $Y^{C}(x)$. Therefore, eq.(\ref{eq:data_term}) can be written as

\begin{equation}
    \sum_{x\in\Omega_{R}} \int_{\mathcal{Y}^{C}(x)} \left[\int_{\mathbf{R}} log f_{x}\left(\ell - I(\phi^{-1}_{I}(x))\right) P(Y(x) = \ell | I, J, \phi^{(0)})d\ell\right]  dP(y^{C}(x) | I, J, \phi^{(0)})
\end{equation}
where $\mathcal{Y}^{C}(x)$ is the set of all possible configurations of $Y^{C}(x)$.\\

Since \hbox{$\int_{\mathcal{Y}^{C}(x)}dP(y^{C}(x) | I, J, \phi^{(0)}) = 1$}, and the inner integral
is the expected value of the given logarithm w.r.t. the conditional distribution, we have
\begin{equation}
     Q(\phi_{I}; \phi^{(0)}) + R(\phi_{I}) = \sum_{x\in\Omega_{R}} \mathbbm{E} \left[\left.\log f_{x}\left(Y(x) - I(\phi^{-1}_{I}(x))\right) \right| I, J, \phi^{(0)}\right].
\end{equation}
By expanding the density function (according to eq. \ref{eq:gaussian}) and denoting by $\overline{Y}(x), \widehat{\sigma^{2}_{Y}(x)}$ the conditional mean and variance of $Y(x)$
given the data and fixed parameters $\phi^{(0)}$, we have
\begin{equation}
    \mathbbm{E} \left[\left.\log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \frac{(Y(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}\right|I, J, \phi^{(0)} \right] =
\end{equation}
\begin{equation}
    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \mathbbm{E} \left[\left. \frac{(Y(x) - \overline{Y}(x) + \overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}\right|I, J, \phi^{(0)} \right]=
\end{equation}
%\begin{equation}
%    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \mathbbm{E} \left[\left. \frac{(Y(x) - \overline{Y}(x))^{2}+ (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2} - 2(Y(x) - \overline{Y}(x)) (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))}{2\sigma^{2}_{Y}(x)}\right|I, J, \phi^{(0)} \right]=
%\end{equation}
\begin{equation}
    = \log \frac{1}{\sigma_{Y}(x)\sqrt{2\pi}} - \frac{\widehat{\sigma^{2}_{Y}(x)} + (\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\sigma^{2}_{Y}(x)}.
\end{equation}
Finally, by approximating the (unknown) parameters $\sigma_{Y}(x)$ by $\widehat{\sigma_{Y}(x)}$ we obtain the expectation of the log-likelihood to be \textbf{maximized} w.r.t $\phi_{I}$ in the M step:
\begin{equation}
    Q(\phi_{I}; \phi^{(0)}) = C_{I} - \sum_{x\in\Omega_{x}}\frac{(\overline{Y}(x) - I(\phi^{-1}_{I}(x)))^{2}}{2\widehat{\sigma^{2}_{Y}(x)}} - R(\phi_{I}),
\end{equation}
where $C_{I} = -\frac{1}{2} - \log\left(\widehat{\sigma^{2}_{Y}(x)}\sqrt{2\pi}\right)$ is a constant that depends on the (known) initial transformations $\phi^{(0)}$, but not on the
(unknown) transformation $\phi_{I}$ we wish to optimize. Image $\overline{Y}$ may be regarded as the expected image $\tilde{J}$ under the modality of $\tilde{I}$, given the data and current estimate of the
deformations $\phi^{(0)}$, denoted $\mathbbm{E}\left[F_{J}[\tilde{J}]|I, J, \phi^{(0)}\right]$ or simply $\mathbbm{E}\left[F_{J}[\tilde{J}]\right]$, and $\widehat{\sigma^{2}_{Y}}(x)$ measures the uncertainty at each point
$x\in \Omega_{R}$.\\

Analogously, if $\overline{Z}(x), \widehat{\sigma^{2}_{Z}(x)}$ are the conditional mean and variance of $Z(x)$ given the data
and fixed parameters $\phi^{(0)}$, the cost function associated to $Z$ is
\begin{equation}
    Q(\phi_{J}; \phi^{(0)}) = C_{J} - \sum_{x\in\Omega_{x}}\frac{(\overline{Z}(x) - J(\phi^{-1}_{J}(x)))^{2}}{2\widehat{\sigma^{2}_{Z}(x)}} - R(\phi_{J}).
\end{equation}


\pagebreak
\section{Gradient of the CC metric}\label{ap:CC_gradient}
We are interested in computing the gradient of
\begin{equation}
    CC(\bar{I}, \bar{J}, x) = \frac{<\bar{I}, \bar{J}>^{2}}{<\bar{I}><\bar{J}>}
\end{equation}
where the inner product is taken over an $n^{D}$ window (eq. 4, in Avants et al.\cite{Avants2008}). Since the windows are considered discrete, a more precise notation
for this expression is:
\begin{equation}
    CC(y;\phi_{I}, \phi_{J}) = \frac{\left[\sum_{z\in W_{y}} \left(\tilde{I}(z) - \mu_{y}\right)\left(\tilde{J}(z) - \nu_{y}\right)\right]^{2}}
    {\left[\sum_{z \in W_{y}}\left(\tilde{I}(z) - \mu_{y}\right)^{2}\right] \left[\sum_{z \in W_{y}}\left(\tilde{J}(z) - \nu_{y}\right)^{2}\right]} = \frac{A_{y}^{2}}{B_{y}C_{y}}
\end{equation}
where $\tilde{I}(z) = I(\phi_{I}^{-1}(z))$, $\tilde{J}(z) = J(\phi_{J}^{-1}(z))$ and the full energy is given by
\begin{equation}
    CC(\phi_{I}, \phi_{J}) = \sum_{y\in\Omega} CC(y; \phi_{I}, \phi_{J})
\end{equation}
where $W_{y}$ is the window of side $n$ centered at voxel $y$, $|W_{y}|$ is the number of voxels in window $W_{y}$ and:
\begin{equation}
    \begin{array}{lll}
        \mu_{y} &=& \frac{1}{|W_{y}|}\sum_{z \in W_{y}}\tilde{I}(z)\\
        \nu_{y} &=& \frac{1}{|W_{y}|}\sum_{z \in W_{y}}\tilde{J}(z)\\
    \end{array}.
\end{equation}

We wish to compute the gradient of $CC(\phi_{I}, \phi_{J})$ with respect to $\phi^{-1}_{J}(x)$, $x\in\Omega$. The set of window terms $CC(y;\phi_{I}, \phi_{J})$
that depend on $\phi^{-1}_{J}(x)$ is precisely the set of all windows $W_{y}$ such that $y \in W_{x}$. Therefore:
\begin{equation}
    \frac{\partial CC (\phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} = \sum_{y \in W_{x}} \frac{\partial CC (y; \phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)}
\end{equation}
and
\begin{equation}
    \frac{\partial CC (y; \phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} =
        \frac{\left(2A_{y} B_{y}C_{y}\right)\frac{\partial A_{y}}{\partial \phi^{-1}_{J}(x)} - \left(A_{y}^{2}B_{y}\right)\frac{\partial C_{y}}{\partial \phi^{-1}_{J}(x)}}
             {B_{y}^{2} C_{y}^{2}}
\end{equation}
where
\begin{equation}
    \begin{array}{lll}
        \frac{\partial A_{y}}{\partial \phi^{-1}_{J}(x)} &=& (\tilde{I}(x) - \mu_{y})\nabla \tilde{J}(x)\\
        \frac{\partial C_{y}}{\partial \phi^{-1}_{J}(x)} &=& 2(\tilde{J}(x) - \nu_{y})\nabla \tilde{J}(x)
    \end{array}.
\end{equation}
After simplifying, we get
\begin{equation}\label{eq:CC_gradient}
    \frac{\partial CC (\phi_{I}, \phi_{J})}{\partial \phi^{-1}_{J}(x)} = \sum_{y \in W_{x}}
         \frac{2A_{y}}
              {B_{y}C_{y}}\left[ (\tilde{I}(x) - \mu_{y}) - \frac{A_{y}}{C_{y}}\left(\tilde{J}(x) - \nu_{y}\right)\right]\nabla \tilde{J}(x).
\end{equation}
If we define the scalar functions
\begin{equation}
    \begin{array}{lll}
        S_{a}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}}{B_{y}C_{y}}\\
        S_{b}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}^{2}}{B_{y}C_{y}^{2}}\\
        S_{c}(x) &=& \sum_{y \in W_{x}} \frac{2A_{y}}{B_{y}C_{y}} \left[ \mu_{y} - \frac{A_{y}}{C_{y}}\nu_{y}\right].
    \end{array}
\end{equation}
we can write the gradient of $CC$ with respect to $\phi^{-1}_{J}$ as:

\begin{equation}
    \psi_{J} = \nabla_{\phi^{-1}_{J}} CC(\phi_{I}, \phi_{J}) = \left[S_{a} \tilde{I} - S_{b}\tilde{J} - S_{c}\right]\nabla \tilde{J}.
\end{equation}
