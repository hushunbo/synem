\section{Discussion}
The matching functional presented in this work reduces the multi-modal problem to the mono-modal case by estimating two transfer functions between the two modalities. The main novelties introduced by our functional are: 1) it models the registration problem symmetrically, which allows us to use both transfer functions instead of selecting one of them arbitrarily, and 2) it computes two global transfer functions that makes the local relationship between image modalities as close to affine as possible. The estimation of both transfer functions is naturally introduced into the formulation of the SyN transformation model developed by Avants {\it et al.}\cite{Avants2008, Avants2011}, which may be regarded as a Generalized EM algorithm (GEM) \citep{Neal1998}, in which the energy is not fully optimized at each step with respect to the transformations $\phi_{I}, \phi_{J}$, but only reduced at each iteration. It has been shown that a full optimization in the maximization step is not necessary, and a local maximum of the likelihood is still reached with partial optimizations \citep{Neal1998}. The greedy SyN algorithm performs two displacement field inversions at each iteration (lines 7, 8 of algorithm \ref{alg:Greedy_SyN}), which may be regarded as projections to the space of diffeomorphisms. It is important to note that by directly inverting the displacement fields (using a variation of Chen's algorithm \cite{Chen2008}), the resulting transformations will be diffeomorphisms \footnote{Approximately diffeomorphisms, since there will always be numerical precision issues, regardless of how the inverse consistency is promoted.}\cite{Avants2008, Avants2011}. Therefore, it is unnecessary to introduce any constraint on the local Jacobians to ensure inverse consistency. Technically, it would be necessary to show that the two inversions effectively produce the projection of the updated displacement field to the space of diffeomorphisms, that way the optimization process of SyN might be interpreted as a gradient projection method \citep{Xiu2007}. However, the transformation model is not a contribution of this work, but only the matching functional for multi-modal registration.\\

The validation protocol proposed in this work allows us to quantitatively assess the accuracy of multi-modal registration algorithms under realistic conditions for three different image modalities and their combinations: T1, T2 and PD. It is very important to note that, although the way we generate the semi-synthetic images may appear to be tailored towards our model (applying a transfer function to the real T1 images), in other words, the so-called ``inverse crime'' that may bias the results towards the proposed method, this is not the case. Please note that in order to generate a semi-synthetic image, we compute a transfer function mapping intensities from each real T1 image to intensities of the warped T2/PD template (i.e. intensities of each real image are transformed using a different transfer function, independent from each other). Afterwards, the warped template is simply discarded, and not used for evaluation any more (it was used just to obtain a ``realistic'' transfer function). Images from the same subject (real and synthetic) are never registered to each other during evaluation. Therefore, there does not exist, in general, any transfer function mapping intensities between any pair of images registered during validation, which means that our model does not have any advantage under the proposed evaluation. Still, we did not exploit the full potential of this validation protocol: it is also possible to study the performance of the same multi-modal registration algorithms in the presence of noise and spatial inhomogeneities, since the Brainweb template allows us to realistically simulate this kind of distortions. A more comprehensive comparison of other available non-linear multi-modal algorithms including these artifacts is a subject for future research. For the full validation, it was necessary to perform 2,754 registrations for each of the 4 methods under evaluation (a total of 11,016 registrations).

\section{Conclusion}
We presented a new matching functional, which we call Expected Cross Correlation (ECC), for multi-modal image registration. We evaluated our matching functional using the publicly available IBSR database following the methodology of Klein {\it et al.}\cite{Klein2009, Klein2010} and Rohlfing \cite{Rohlfing2012} to show that it is competitive for mono-modal image registration. To evaluate the performance of ECC for multi-modal registration in a realistic, controlled, experiment we used the Brainweb \citep{Cocosco1997, Kwan1999} template to generate synthetic T2 and PD images for each of the IBSR T1 volumes. Experiments show that the CC metric is dramatically affected by the change of modality while the performance of ECC is less affected, remaining comparable to the mono-modal case and comparing favourably to the reference implementation of SyN with Mutual Information provided by the ANTS software. Our algorithms are publicly available in DIPY \citep{Garyfallidis2014}.\\
